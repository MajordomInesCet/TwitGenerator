{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, CuDNNLSTM, Dense, LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Merge, Dot, Concatenate, Flatten, Permute, Multiply, dot, concatenate\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle, choice, sample\n",
    "import time\n",
    "\n",
    "import pprint as pp\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_EMBEDDINGS = True\n",
    "LOAD_PKL_DATA = False\n",
    "SAMPLE_EVERY = 3\n",
    "PLOT_EVERY = 3\n",
    "MAX_LOOKUP_LEN = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_dataset = pickle.load(open(os.path.join(data_path, 'tweets_together.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "tokenized = [list(x) for x in spam_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_token = [s[1] for s in tokenized if len(s) > 1]\n",
    "len(start_token)\n",
    "\n",
    "maxlen = max([len(x) for x in tokenized])\n",
    "avglen = sum([len(x) for x in tokenized])/len(tokenized)\n",
    "print(maxlen, avglen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [t for s in spam_dataset for t in s]\n",
    "print('num tokens: ', len(vocab))\n",
    "vocab_counter = Counter(vocab)\n",
    "vocab = [w for w, v in vocab_counter.items() if v>2]\n",
    "vocab = ['<PAD>','<UNK>', '<SOS>', '<EOS>'] + vocab\n",
    "nb_vocab = len(vocab)\n",
    "print('Num de features a usar: ', nb_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2id = {w:i for i, w in enumerate(vocab)}\n",
    "id2w = {w:i for i, w in w2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = min(maxlen, MAX_LOOKUP_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_chars = [x[:maxlen] for x in tokenized]\n",
    "for i in range(len(init_chars)):\n",
    "    tmp = init_chars[i]\n",
    "    tmp.insert(0, '<SOS>')\n",
    "    init_chars[i] = tmp[:maxlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', '@miriamnoguerasM', 'Gràcies,', 'Míriam.', '<EOS>']\n",
      "['@miriamnoguerasM', 'Gràcies,', 'Míriam.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "data_train = []\n",
    "for x in tokenized:\n",
    "    x.insert(0, '<SOS>')\n",
    "    x.append('<EOS>')\n",
    "    for i in range(0, len(x) - maxlen, step):\n",
    "        data_train.append((x[i: i + maxlen], x[i + maxlen]))\n",
    "\n",
    "print('nb sequences:', len(data_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_pred(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sampletest(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % SAMPLE_EVERY == 0  and epoch>0:\n",
    "            data_test = []\n",
    "            nb_samples = 1\n",
    "\n",
    "            params = {\n",
    "                'maxlen': maxlen,\n",
    "                'vocab': nb_vocab,\n",
    "                'use_embeddings': True\n",
    "                }\n",
    "            for _ in range(nb_samples):\n",
    "                data_test = choice(init_chars)\n",
    "                x_pred = np.zeros((1, params['maxlen'], params['vocab']), dtype=np.bool)\n",
    "                for diversity in [0.1, 0.3, 0.5, 0.7, 1.0, 1.2]:\n",
    "                    print('----- diversity:', diversity)\n",
    "                    sentence = copy(data_test)\n",
    "                    generated = copy(data_test)\n",
    "                    for i in range(len(data_test), 140):\n",
    "                        x_pred = np.zeros((1, params['maxlen'], params['vocab']))\n",
    "                        for t, char in enumerate(sentence):\n",
    "                            x_pred[0, t, w2id[char] if char in w2id else w2id['<UNK>']] = 1.\n",
    "                        preds = self.model.predict(x_pred, verbose=0)[0]\n",
    "                        next_index = sample_pred(preds, diversity)\n",
    "                        next_char = id2w[next_index]\n",
    "                        if next_char == '<EOS>':\n",
    "                            break\n",
    "                        generated += [next_char]\n",
    "                        sentence = sentence[1:]\n",
    "                        sentence += [next_char]\n",
    "                    print(''.join(generated))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HistoryDisplay(Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "        self.epochs = []\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        #plt.show()\n",
    "        \n",
    "        plt.ion()\n",
    "        self.fig.show()\n",
    "        self.fig.canvas.draw()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.epochs.append(epoch)\n",
    "        self.losses.append(logs['loss'])\n",
    "        self.accs.append(logs['acc'])\n",
    "        if epoch % PLOT_EVERY == 0:\n",
    "            \n",
    "            self.ax.clear()\n",
    "            self.ax.plot(self.epochs, self.accs, 'g', label='acc')\n",
    "            self.ax.plot(self.epochs, self.losses, 'b', label='loss')\n",
    "            legend = self.ax.legend(loc='upper right', shadow=True, fontsize='x-large')\n",
    "            #display.clear_output(wait=True)\n",
    "            #display.display(pl.gcf())\n",
    "            self.fig.canvas.draw()\n",
    "            \n",
    "            #plt.draw()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LM:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs.pop('params', None)\n",
    "\n",
    "    def compile_bidirectional(self, params={}):\n",
    "\n",
    "\n",
    "        encoder_inputs = Input(shape=(params['maxlen'], params['vocab']), name='encoder_input')\n",
    "        encoder_embedding = encoder_inputs\n",
    "\n",
    "        #print(encoder_embedding.shape)\n",
    "\n",
    "        lstm = CuDNNLSTM(params['rnn_hidden_size'], return_sequences=True, name='rnn_1')\n",
    "        if 'bidirectional' in params and params['bidirectional']:\n",
    "            encoder_outputs = Bidirectional(lstm)(encoder_embedding)\n",
    "            lstm2 = CuDNNLSTM(params['rnn_hidden_size'], return_sequences=False, name='rnn_1')\n",
    "            encoder_outputs = Bidirectional(lstm2)(encoder_outputs)\n",
    "        else:\n",
    "            encoder_outputs = lstm(encoder_embedding)\n",
    "\n",
    "        decoder_outputs = Dense(params['vocab'], activation=\"softmax\")(encoder_outputs)\n",
    "        # Define the model that will turn\n",
    "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "        model = Model(encoder_inputs, decoder_outputs)\n",
    "\n",
    "        # Run training\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self, model, data, params={}):\n",
    "\n",
    "        callbacks = self._get_callbacks()\n",
    "        print(callbacks)\n",
    "        if 'shuffle' in params and params['shuffle']:\n",
    "            shuffle(data)\n",
    "        print(data[0])\n",
    "        sentences, next_chars = zip(*data)\n",
    "        x = np.zeros((len(data), params['maxlen'], params['vocab']), dtype=np.bool)\n",
    "        y = np.zeros((len(data), params['vocab']), dtype=np.bool)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[i, t, w2id[char] if char in w2id else w2id['<UNK>']] = 1\n",
    "            y[i, w2id[next_chars[i]] if next_chars[i] in w2id else w2id['<UNK>']] = 1\n",
    "        model.fit(x, y,\n",
    "        batch_size=params['batch_size'],\n",
    "        epochs=params['epochs'],\n",
    "        callbacks=callbacks,\n",
    "                 verbose=1)\n",
    "\n",
    "    def predict(self, model, data, params={}):\n",
    "        \"\"\"Short summary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : type\n",
    "            Description of parameter `model`.\n",
    "        data : type\n",
    "            Description of parameter `data`.\n",
    "        params : type\n",
    "            Description of parameter `params`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        type\n",
    "            Description of returned object.\n",
    "\n",
    "        \"\"\"\n",
    "        x_pred = np.zeros((1, params['maxlen'], params['vocab']), dtype=np.bool)\n",
    "        y_pred = []\n",
    "        for diversity in [0.1, 0.3, 0.5, 0.7, 1.0, 1.2]:\n",
    "            sentence = copy(data)\n",
    "            generated = copy(data)\n",
    "            for i in range(len(data), 200):\n",
    "                x_pred = np.zeros((1, params['maxlen'], params['vocab']))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, w2id[char] if char in w2id else w2id['<UNK>']] = 1.\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample_pred(preds, diversity)\n",
    "                next_char = id2w[next_index]\n",
    "                if next_char == '<EOS>':\n",
    "                    break\n",
    "                generated += [next_char]\n",
    "                sentence = sentence[1:]\n",
    "                sentence += [next_char]\n",
    "            y_pred.append(''.join(generated))\n",
    "        return y_pred\n",
    "\n",
    "    def load(self, model_path='lminesymayordomo_7.h5'):\n",
    "        print('MODEL PATH: ', model_path)\n",
    "        return load_model(model_path)\n",
    "\n",
    "    def _get_callbacks(self, model_path='lm'):\n",
    "        model_path = '{}.h5'.format(args.file)\n",
    "        es = EarlyStopping(monitor='loss', patience=4, mode='auto', verbose=0)\n",
    "        save_best = ModelCheckpoint(model_path, monitor='loss', verbose = 0, save_best_only=True, save_weights_only=False, period=2)\n",
    "        st = Sampletest()\n",
    "        rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=0)\n",
    "        hd = HistoryDisplay()\n",
    "        return [st, save_best, hd]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOAD_MODEL = False\n",
    "bTrain = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bidirectional': True,\n",
      " 'dropout': 0.9,\n",
      " 'emb_out_sz': 50,\n",
      " 'input_size': 3833,\n",
      " 'max_decoder_len': 55,\n",
      " 'max_encoder_len': 55,\n",
      " 'rnn_dropout': 0.9,\n",
      " 'rnn_hidden_size': 128,\n",
      " 'target_size': 3833,\n",
      " 'use_embeddings': True,\n",
      " 'vocab': 3833}\n"
     ]
    }
   ],
   "source": [
    "lm = LM()\n",
    "if LOAD_MODEL:\n",
    "    compile_params = {\n",
    "        'vocab': nb_vocab,\n",
    "        'rnn_hidden_size': 512,\n",
    "        'maxlen': maxlen,\n",
    "        'use_embeddings': True,\n",
    "        'bidirectional': True,\n",
    "        'dropout': 0.6,\n",
    "        'rnn_dropout': 0.6\n",
    "    }\n",
    "    pp.pprint(compile_params)\n",
    "    compile_params['w2id'] = w2id\n",
    "    compile_params['id2w'] = id2w\n",
    "    pickle.dump(compile_params, open('{}_config.pkl'.format(args.file), 'wb'))\n",
    "    lm_model = lm.compile_bidirectional(params=compile_params)\n",
    "else:\n",
    "    model_path = '{}.h5'.format(args.file)\n",
    "    lm_model = lm.load(model_path=model_path)\n",
    "    compile_params = pickle.load(open('{}_config.pkl'.format(args.file), 'rb'))\n",
    "    w2id = compile_params.pop('w2id')\n",
    "    id2w = compile_params.pop('id2w')\n",
    "    pp.pprint(compile_params)\n",
    "    lm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "embedding_layer_encoder (Emb (None, 55, 50)            191650    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 55, 256)           184320    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 55, 3833)          985081    \n",
      "=================================================================\n",
      "Total params: 1,361,051\n",
      "Trainable params: 1,361,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if bTrain:\n",
    "    train_params = {\n",
    "        'epochs': 300,\n",
    "        'batch_size': 128,\n",
    "        'shuffle': True,\n",
    "        'vocab': nb_vocab,\n",
    "        'maxlen': maxlen,\n",
    "        'use_embeddings': True\n",
    "    }\n",
    "    pp.pprint(train_params)\n",
    "    lm.train(model=lm_model, data=data_train, params=train_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(args.n_predict):\n",
    "    if args.warm_up:\n",
    "        data_pred = choice(init_chars)\n",
    "    else:\n",
    "        data_pred = input('warm up chars (Max used chars {})'.format(compile_params['maxlen']))\n",
    "        data_pred = list(data_pred)[:compile_params['maxlen']]\n",
    "    y_pred = lm.predict(model=lm_model, data=data_pred, params=compile_params)\n",
    "    for s in y_pred:\n",
    "        f.write('\\n{}\\n'.format(s))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
